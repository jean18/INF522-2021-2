{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "TM_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jean18/INF522-2021-2/blob/main/Jupyter%20Notebooks/TM_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HThYrqCC0zDR"
      },
      "source": [
        "<img src=\"https://www.inf.utfsm.cl/images/slides/Departamento-de-Informtica_HORIZONTAL.png\" title=\"Title text\" width=\"80%\" />\n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "<h1 align='center'> INF-522 Text Mining 2021-2 </h1>\n",
        "\n",
        "<H3 align='center'> Tema 1 - Fundamentos de text mining </H3>\n",
        "<hr style=\"height:2px;border:none\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4NUb865Ceyk"
      },
      "source": [
        "## Librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbdVibZTtFdx"
      },
      "source": [
        "# Librerias\n",
        "\n",
        "import nltk \n",
        "import spacy\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer # Porter 2\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toYdJiOvuwCw",
        "outputId": "65299f36-5897-4be5-b723-4889d7bd6fa4"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HTGaoUjCh5M"
      },
      "source": [
        "## Vocabulario\n",
        "\n",
        "##### Stemming:\n",
        "\n",
        "*   Etiquetar cada termino de acuerdo a la funcion que este cumple en el texto.\n",
        "*   Puede ayudarnos en tareas como deteccion de estilo, parsing, deteccion de colocaciones.\n",
        "*   Tarea importante en NLP.\n",
        "\n",
        "##### ¿Cómo funciona?\n",
        "\n",
        "*   Se dispone de un corpus etiquetado.\n",
        "*   La secuencia de tags es interpretada como una cadena de Markov: $P(x_{t+1} |x_{t}, ..., x{1}) = P(x_{t+1} | x_{t})$\n",
        "*   $x_{1}, ..., x_{t+1}$ representan tags.\n",
        "*   Usamos un modelo generativo para términos, con tags como estados ocultos: $P(t|x_{1}, ..., x_{t+1}) = P(t|x_{t+1})$\n",
        "*   En general muestran buena precision (sobre 90%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBLc3-m4tY3G"
      },
      "source": [
        "## Stemming en NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GgdPK3ottNf",
        "outputId": "3480adf8-bb66-43ec-dc7d-bae6186b6812"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
        "\n",
        "for w in words:\n",
        "  print(w, \" : \", ps.stem(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programmer  :  programm\n",
            "programming  :  program\n",
            "programmers  :  programm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBD4i5-quRF2",
        "outputId": "527127c7-d6c1-473f-9342-31537085d9d3"
      },
      "source": [
        "sentence = \"Programmers program with programming languages\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "for w in words:\n",
        "  print(w, \" : \", ps.stem(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Programmers  :  programm\n",
            "program  :  program\n",
            "with  :  with\n",
            "programming  :  program\n",
            "languages  :  languag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7csjwTivA3T"
      },
      "source": [
        "## Wordnet lemmatizer en NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCMaCGhJvPwC",
        "outputId": "66a8cdf5-8a37-4238-8a4f-99ad5c564492"
      },
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling', 'driving', 'died', 'tried', 'feet']\n",
        "\n",
        "for word in list1:\n",
        "  print(word + \" ------> \" + wnl.lemmatize(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kites ------> kite\n",
            "babies ------> baby\n",
            "dogs ------> dog\n",
            "flying ------> flying\n",
            "smiling ------> smiling\n",
            "driving ------> driving\n",
            "died ------> died\n",
            "tried ------> tried\n",
            "feet ------> foot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFx9CYpA0-3w",
        "outputId": "1fca650c-d664-4464-fcc6-0db31a2fcbb7"
      },
      "source": [
        "string = 'the cat is sitting with the bats on the striped mat under many flying geese'\n",
        "\n",
        "list2 =  word_tokenize(string)\n",
        "\n",
        "print(list2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on', 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY_w8dY7x0eM",
        "outputId": "f2c3ba63-123e-4913-ba6d-20aa8b2661b4"
      },
      "source": [
        "lemmatized_string = ' '.join([wnl.lemmatize(words) for words in list2])\n",
        "\n",
        "print(lemmatized_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the cat is sitting with the bat on the striped mat under many flying goose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bRNGdQsyzsk"
      },
      "source": [
        "## NLP en español"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVlwACJDyYoo",
        "outputId": "f65c37c9-fa38-4aa4-a75f-9038a53bc7e2"
      },
      "source": [
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "text = \"\"\"Soy un texto. Normalmente soy más largo y más grande. Que no te engañe mi tamaño.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "lexical_tokens = [t.orth_ for t in doc if not t.is_punct | t.is_stop]\n",
        "\n",
        "print(lexical_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['texto', 'Normalmente', 'y', 'grande', 'engañe', 'tamaño']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX1-sBC70DND",
        "outputId": "79c2fc29-b5a4-47b8-a82e-e0f36c082eaf"
      },
      "source": [
        "def normalize(text):\n",
        "\n",
        "  doc = nlp(text)\n",
        "  words = [t.orth_ for t in doc if not t.is_punct | t.is_stop]\n",
        "  lexical_tokens = [t.lower() for t in words if len(t) > 3 and t.isalpha()]\n",
        "\n",
        "  return lexical_tokens\n",
        "\n",
        "\n",
        "text = \"Soy un texto de prueba. ¿Cuántos tokens me quedaran después de la normalización?\"\n",
        "\n",
        "tokens = normalize(text)\n",
        "\n",
        "print(tokens) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['texto', 'prueba', 'tokens', 'quedaran', 'normalización']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JChDPXVG0Yls",
        "outputId": "faca259b-2af4-4f62-898e-5fed70fa4e3d"
      },
      "source": [
        "spanish_stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "text = \"\"\"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…\"\"\"\n",
        "\n",
        "tokens = normalize(text) # Crear una lista de tokens\n",
        "\n",
        "stems = [spanish_stemmer.stem(token) for token in tokens]\n",
        "\n",
        "print(stems)\n",
        "\n",
        "# Tambien tiene lematizacion y otros modulos del pipeline NLP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['text', 'pid', 'grit', 'proces', 'cant', 'cant', 'cant', 'cant', 'cant', 'cant']\n"
          ]
        }
      ]
    }
  ]
}